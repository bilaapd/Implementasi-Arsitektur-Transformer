ğŸ“˜ Transformer From Scratch Using NumPy Implementation

Project ini adalah implementasi decoder-only Transformer (GPT-style) dari nol menggunakan NumPy.

Fokus utama adalah forward pass:

Input berupa token ID sederhana

Proses embedding + positional encoding

Multi-Head Attention dengan causal masking

Feed-Forward Network (FFN)

Residual connection + LayerNorm

Output berupa logits [batch, seq_len, vocab_size]

Distribusi probabilitas token berikutnya dengan softmax

âš™ï¸ Fitur yang Diimplementasikan

âœ… Token Embedding

âœ… Positional Encoding

âœ… Scaled Dot-Product Attention

âœ… Multi-Head Attention 

âœ… Feed-Forward Network 

âœ… Residual Connection + LayerNorm 

âœ… Causal Masking

âœ… Output Layer ke vocab + softmax

ğŸ› ï¸ Cara Menjalankan
1. Clone Repository
git clone https://github.com/username/TransformerFromScratch.git
cd TransformerFromScratch

2. Install Dependencies

Pastikan sudah ada Python 3.9+. Lalu install NumPy:

pip install numpy

3. Jalankan Forward Pass Test
python test_forward.py

